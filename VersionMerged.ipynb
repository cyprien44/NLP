{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Sentiment\n",
      "0       neutral\n",
      "1      positive\n",
      "2      positive\n",
      "3      positive\n",
      "4      negative\n",
      "...         ...\n",
      "22519  positive\n",
      "22520   neutral\n",
      "22521  positive\n",
      "22522  positive\n",
      "22523   neutral\n",
      "\n",
      "[22524 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Charger le dataset\n",
    "dataset = pd.read_csv(\"C:/Users/flavi/Downloads/fifa_world_cup_2022_tweets.csv\")\n",
    "\n",
    "# print dataset info to see how the dataset is structured\n",
    "print(dataset[['Sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        What are we drinking today @TucanTribe \\n@MadB...\n",
      "1        Amazing @CanadaSoccerEN  #WorldCup2022 launch ...\n",
      "2        Worth reading while watching #WorldCup2022 htt...\n",
      "3        Golden Maknae shinning bright\\n\\nhttps://t.co/...\n",
      "4        If the BBC cares so much about human rights, h...\n",
      "                               ...                        \n",
      "22519              Here We go World cup 2022 #WorldCup2022\n",
      "22520    Anderlecht confirms former Viborg FF's Jesper ...\n",
      "22521    Great thread to read before the start of #Worl...\n",
      "22522    Raphinha wants Brazil to be united at the #Wor...\n",
      "22523    How to buy $SOT on PinkSale?ðŸ¤”\\n\\nHave you been...\n",
      "Name: Tweet, Length: 22524, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# # separate tweets and sentiments\n",
    "# tweets = dataset['Tweet']\n",
    "# labels = dataset['Sentiment']\n",
    "# print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        drinking today tucantribe madbears lkincalgo a...\n",
      "1        amazing canadasocceren worldcup2022 launch vid...\n",
      "2        worth reading watching worldcup2022 stco1sqrna...\n",
      "3        golden maknae shinning bright stco4ayzbzgtx4 j...\n",
      "4        bbc cares much human rights homosexual rights ...\n",
      "                               ...                        \n",
      "22519                       go world cup 2022 worldcup2022\n",
      "22520    anderlecht confirms former viborg ffs jesper f...\n",
      "22521    great thread read start worldcup2022 stcovp62j...\n",
      "22522    raphinha wants brazil united worldcup2022 stco...\n",
      "22523    buy sot pinksale confused buy tokens pinksale ...\n",
      "Name: Tweet, Length: 22524, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess tweets\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.replace('\\n', ' ')  # Replace newline characters with spaces\n",
    "    tweet = tweet.replace('\\r', ' ')  # Replace carriage returns with spaces\n",
    "    tweet = tweet.replace('http', '')  # Remove URLs starting with 'http'\n",
    "    tweet = tweet.replace('www', '')  # Remove URLs starting with 'www'\n",
    "    tweet = tweet.replace('https', '')  # Remove URLs starting with 'https'\n",
    "    tweet = tweet.replace('@', '')  # Remove '@' symbols\n",
    "    tweet = tweet.replace('#', '')  # Remove '#' symbols\n",
    "    tweet = ''.join([char for char in tweet if char.isalnum() or char.isspace()])  # Remove special characters\n",
    "    tweet = tweet.lower()  # Convert to lowercase\n",
    "    tweet = ' '.join([word for word in tweet.split() if word not in stop_words])  # Remove stopwords\n",
    "    return tweet\n",
    "\n",
    "tweets_df = dataset['Tweet'].apply(preprocess_tweet)\n",
    "    \n",
    "\n",
    "print(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(tweets_df)\n",
    "X = tokenizer.texts_to_sequences(tweets_df)\n",
    "\n",
    "# Padding sequences\n",
    "X = pad_sequences(X, maxlen=100)\n",
    "\n",
    "# Convert sentiment labels to numerical values\n",
    "dataset['Sentiment'] = dataset['Sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
    "y = dataset['Sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "282/282 [==============================] - 1495s 5s/step - loss: 0.8099 - accuracy: 0.6140 - val_loss: 0.6549 - val_accuracy: 0.7085\n",
      "Epoch 2/5\n",
      "282/282 [==============================] - 1449s 5s/step - loss: 0.5657 - accuracy: 0.7604 - val_loss: 0.6461 - val_accuracy: 0.7194\n",
      "Epoch 3/5\n",
      "282/282 [==============================] - 1800s 6s/step - loss: 0.4795 - accuracy: 0.8053 - val_loss: 0.6876 - val_accuracy: 0.7145\n",
      "Epoch 4/5\n",
      "282/282 [==============================] - 897s 3s/step - loss: 0.4276 - accuracy: 0.8305 - val_loss: 0.7123 - val_accuracy: 0.7170\n",
      "Epoch 5/5\n",
      "282/282 [==============================] - 1000s 4s/step - loss: 0.3782 - accuracy: 0.8493 - val_loss: 0.7530 - val_accuracy: 0.7148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x236d6738150>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Building the custom LSTM\n",
    "\n",
    "class CustomLSTMModel:\n",
    "    def __init__(self, vocab_size, embedding_dim, input_length, hidden_dim, output_size):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length))\n",
    "        self.model.add(SpatialDropout1D(0.2))\n",
    "        self.model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
    "        self.model.add(LSTM(hidden_dim, dropout=0.2, recurrent_dropout=0.2))\n",
    "        self.model.add(Dense(output_size, activation='softmax'))\n",
    "        \n",
    "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "    def fit(self, X_train, y_train, validation_data, epochs=5, batch_size=64):\n",
    "        history = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data, verbose=1)\n",
    "        return history\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        loss, accuracy = self.model.evaluate(X_test, y_test, verbose=1)\n",
    "        return loss, accuracy\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test)\n",
    "\n",
    "\n",
    "# Ensure the data is a numpy array\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the custom LSTM model\n",
    "vocab_size = 5000  # This can be adjusted based on the tokenizer settings\n",
    "embedding_dim = 128\n",
    "input_length = 100\n",
    "hidden_dim = 100\n",
    "output_size = 3  # For three classes: negative, neutral, positive\n",
    "\n",
    "custom_lstm_model = CustomLSTMModel(vocab_size, embedding_dim, input_length, hidden_dim, output_size)\n",
    "custom_lstm_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 9s 66ms/step - loss: 0.7530 - accuracy: 0.7148\n",
      "Test Loss: 0.7529652118682861 | Test Accuracy: 0.7147613763809204\n",
      "141/141 [==============================] - 16s 85ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.74      0.72      1149\n",
      "     neutral       0.68      0.65      0.67      1648\n",
      "    positive       0.75      0.76      0.75      1708\n",
      "\n",
      "    accuracy                           0.71      4505\n",
      "   macro avg       0.71      0.72      0.71      4505\n",
      "weighted avg       0.71      0.71      0.71      4505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = custom_lstm_model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss} | Test Accuracy: {accuracy}')\n",
    "\n",
    "# Predict and print classification report\n",
    "y_pred = np.argmax(custom_lstm_model.predict(X_test), axis=-1)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['negative', 'neutral', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.39200887902330744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.38      0.03      0.06      1149\n",
      "     neutral       0.38      0.27      0.32      1648\n",
      "    positive       0.40      0.75      0.52      1708\n",
      "\n",
      "    accuracy                           0.39      4505\n",
      "   macro avg       0.39      0.35      0.30      4505\n",
      "weighted avg       0.39      0.39      0.33      4505\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flavi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# PAS SUR QUON LE LAISSE MAIS PEUT ETRE INTERESSE POUR MONTRER QUON A TENTE PLUSIEURS MODELES\n",
    "# 3. Model Building : simple regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=['negative', 'neutral', 'positive']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
