{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets transformers\n",
    "\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Sentiment\n",
      "0       neutral\n",
      "1      positive\n",
      "2      positive\n",
      "3      positive\n",
      "4      negative\n",
      "...         ...\n",
      "22519  positive\n",
      "22520   neutral\n",
      "22521  positive\n",
      "22522  positive\n",
      "22523   neutral\n",
      "\n",
      "[22524 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#from datasets import load_dataset\n",
    "import pandas as pd\n",
    "# Charger le dataset\n",
    "dataset = pd.read_csv(\"C:/Users/flavi/Downloads/fifa_world_cup_2022_tweets.csv\")\n",
    "\n",
    "# print dataset info to see how the dataset is structured\n",
    "print(dataset[['Sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        What are we drinking today @TucanTribe \\n@MadB...\n",
      "1        Amazing @CanadaSoccerEN  #WorldCup2022 launch ...\n",
      "2        Worth reading while watching #WorldCup2022 htt...\n",
      "3        Golden Maknae shinning bright\\n\\nhttps://t.co/...\n",
      "4        If the BBC cares so much about human rights, h...\n",
      "                               ...                        \n",
      "22519              Here We go World cup 2022 #WorldCup2022\n",
      "22520    Anderlecht confirms former Viborg FF's Jesper ...\n",
      "22521    Great thread to read before the start of #Worl...\n",
      "22522    Raphinha wants Brazil to be united at the #Wor...\n",
      "22523    How to buy $SOT on PinkSale?ðŸ¤”\\n\\nHave you been...\n",
      "Name: Tweet, Length: 22524, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# separate tweets and sentiments\n",
    "tweets = dataset['Tweet']\n",
    "labels = dataset['Sentiment']\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\flavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\flavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        drinking today tucantribe madbears lkincalgo a...\n",
      "1        amazing canadasocceren worldcup2022 launch vid...\n",
      "2        worth reading watching worldcup2022 stco1sqrna...\n",
      "3        golden maknae shinning bright stco4ayzbzgtx4 j...\n",
      "4        bbc cares much human rights homosexual rights ...\n",
      "                               ...                        \n",
      "22519                       go world cup 2022 worldcup2022\n",
      "22520    anderlecht confirms former viborg ffs jesper f...\n",
      "22521    great thread read start worldcup2022 stcovp62j...\n",
      "22522    raphinha wants brazil united worldcup2022 stco...\n",
      "22523    buy sot pinksale confused buy tokens pinksale ...\n",
      "Name: Tweet, Length: 22524, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess tweets\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.replace('\\n', ' ')  # Replace newline characters with spaces\n",
    "    tweet = tweet.replace('\\r', ' ')  # Replace carriage returns with spaces\n",
    "    tweet = tweet.replace('http', '')  # Remove URLs starting with 'http'\n",
    "    tweet = tweet.replace('www', '')  # Remove URLs starting with 'www'\n",
    "    tweet = tweet.replace('https', '')  # Remove URLs starting with 'https'\n",
    "    tweet = tweet.replace('@', '')  # Remove '@' symbols\n",
    "    tweet = tweet.replace('#', '')  # Remove '#' symbols\n",
    "    tweet = ''.join([char for char in tweet if char.isalnum() or char.isspace()])  # Remove special characters\n",
    "    tweet = tweet.lower()  # Convert to lowercase\n",
    "    tweet = ' '.join([word for word in tweet.split() if word not in stop_words])  # Remove stopwords\n",
    "    return tweet\n",
    "\n",
    "tweets_df = dataset['Tweet'].apply(preprocess_tweet)\n",
    "    \n",
    "\n",
    "print(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(tweets_df)\n",
    "X = tokenizer.texts_to_sequences(tweets_df)\n",
    "\n",
    "# Padding sequences\n",
    "X = pad_sequences(X, maxlen=100)\n",
    "\n",
    "# Convert sentiment labels to numerical values\n",
    "dataset['Sentiment'] = dataset['Sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
    "y = dataset['Sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.src.backend' has no attribute 'convert_to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\progbar.py:162\u001b[0m, in \u001b[0;36mProgbar.update\u001b[1;34m(self, current, values, finalize)\u001b[0m\n\u001b[0;32m    160\u001b[0m info \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[k], \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m--> 162\u001b[0m     avg \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mconvert_to_numpy(\n\u001b[0;32m    163\u001b[0m         backend\u001b[38;5;241m.\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mmean(\n\u001b[0;32m    164\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[k][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[k][\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    165\u001b[0m         )\n\u001b[0;32m    166\u001b[0m     )\n\u001b[0;32m    167\u001b[0m     avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(avg)\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(avg) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1e-3\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.src.backend' has no attribute 'convert_to_numpy'"
     ]
    }
   ],
   "source": [
    "## Building the LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Ensure the data is a numpy array\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7094339622641509\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.67      0.71      1149\n",
      "     neutral       0.64      0.70      0.67      1648\n",
      "    positive       0.75      0.74      0.75      1708\n",
      "\n",
      "    accuracy                           0.71      4505\n",
      "   macro avg       0.72      0.71      0.71      4505\n",
      "weighted avg       0.71      0.71      0.71      4505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Model Building : simple regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=['negative', 'neutral', 'positive']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     tweet \u001b[38;5;241m=\u001b[39m tweet\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tweet\n\u001b[1;32m---> 19\u001b[0m tweets \u001b[38;5;241m=\u001b[39m [preprocess_tweet(tweet) \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweets]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tweets' is not defined"
     ]
    }
   ],
   "source": [
    "## Cleaning and Tokenization:\n",
    "# Remove special characters, URLs, mentions, and hashtags.\n",
    "# Convert text to lowercase.\n",
    "# Tokenize the tweets into words\n",
    "def cleaning_prep_tweets(tweet):\n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
    "    # Remove user @ references and '#' from hashtags\n",
    "    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n",
    "    # Remove special characters, numbers, punctuations\n",
    "    tweet = re.sub(r'\\W', ' ', tweet)\n",
    "    tweet = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', tweet)\n",
    "    tweet = re.sub(r'\\^[a-zA-Z]\\s+', ' ', tweet)\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet, flags=re.I)\n",
    "    tweet = re.sub(r'^b\\s+', '', tweet)\n",
    "    tweet = tweet.lower()\n",
    "    return tweet\n",
    "\n",
    "tweets = [preprocess_tweet(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.src.backend' has no attribute 'convert_to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# EVALUATION DU MODELE\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Classification report\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\progbar.py:162\u001b[0m, in \u001b[0;36mProgbar.update\u001b[1;34m(self, current, values, finalize)\u001b[0m\n\u001b[0;32m    160\u001b[0m info \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[k], \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m--> 162\u001b[0m     avg \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mconvert_to_numpy(\n\u001b[0;32m    163\u001b[0m         backend\u001b[38;5;241m.\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mmean(\n\u001b[0;32m    164\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[k][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[k][\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    165\u001b[0m         )\n\u001b[0;32m    166\u001b[0m     )\n\u001b[0;32m    167\u001b[0m     avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(avg)\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(avg) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1e-3\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.src.backend' has no attribute 'convert_to_numpy'"
     ]
    }
   ],
   "source": [
    "# EVALUATION DU MODELE\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "print(classification_report(y_test, y_pred, target_names=['negative', 'neutral', 'positive']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
