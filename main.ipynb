{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.0.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: transformers==4.30.2 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (4.30.2)\n",
      "Requirement already satisfied: datasets==2.20.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (2.20.0)\n",
      "Requirement already satisfied: pandas==1.5.3 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (1.5.3)\n",
      "Requirement already satisfied: nltk==3.8.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (3.8.1)\n",
      "Requirement already satisfied: scikit-learn==1.2.2 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 7)) (1.2.2)\n",
      "Requirement already satisfied: tensorflow==2.12.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 8)) (2.12.0)\n",
      "Requirement already satisfied: pyarrow==16.1.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 9)) (16.1.0)\n",
      "Requirement already satisfied: numpy==1.23.5 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 10)) (1.23.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.23.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (4.66.4)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0->-r requirements.txt (line 3)) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (3.9.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from pandas==1.5.3->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from pandas==1.5.3->-r requirements.txt (line 4)) (2023.3.post1)\n",
      "Requirement already satisfied: click in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from nltk==3.8.1->-r requirements.txt (line 5)) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from nltk==3.8.1->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 7)) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 7)) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow==2.12.0->-r requirements.txt (line 8)) (2.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (3.9.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.4.29)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.4.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.12.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.31.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (1.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.30.2->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from jinja2->torch==2.0.1->-r requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from sympy->torch==2.0.1->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.41.2)\n",
      "Requirement already satisfied: ml-dtypes>=0.4.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.4.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.30.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.0.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prétraitement des Données\n",
    "\n",
    "* Nous commençons par charger les données depuis un fichier CSV et afficher les informations sur les sentiments pour comprendre la structure des données.\n",
    "* Ensuite, nous définissons une fonction de prétraitement pour nettoyer les tweets en enlevant les caractères spéciaux, les URLs, les mentions et les hashtags, et en supprimant les stopwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are we drinking today @TucanTribe \\n@MadB...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazing @CanadaSoccerEN  #WorldCup2022 launch ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Worth reading while watching #WorldCup2022 htt...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Golden Maknae shinning bright\\n\\nhttps://t.co/...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If the BBC cares so much about human rights, h...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22519</th>\n",
       "      <td>Here We go World cup 2022 #WorldCup2022</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22520</th>\n",
       "      <td>Anderlecht confirms former Viborg FF's Jesper ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22521</th>\n",
       "      <td>Great thread to read before the start of #Worl...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22522</th>\n",
       "      <td>Raphinha wants Brazil to be united at the #Wor...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22523</th>\n",
       "      <td>How to buy $SOT on PinkSale?🤔\\n\\nHave you been...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22524 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Tweet Sentiment\n",
       "0      What are we drinking today @TucanTribe \\n@MadB...   neutral\n",
       "1      Amazing @CanadaSoccerEN  #WorldCup2022 launch ...  positive\n",
       "2      Worth reading while watching #WorldCup2022 htt...  positive\n",
       "3      Golden Maknae shinning bright\\n\\nhttps://t.co/...  positive\n",
       "4      If the BBC cares so much about human rights, h...  negative\n",
       "...                                                  ...       ...\n",
       "22519            Here We go World cup 2022 #WorldCup2022  positive\n",
       "22520  Anderlecht confirms former Viborg FF's Jesper ...   neutral\n",
       "22521  Great thread to read before the start of #Worl...  positive\n",
       "22522  Raphinha wants Brazil to be united at the #Wor...  positive\n",
       "22523  How to buy $SOT on PinkSale?🤔\\n\\nHave you been...   neutral\n",
       "\n",
       "[22524 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#dataset = pd.read_csv(\"C:/Users/flavi/Downloads/fifa_world_cup_2022_tweets.csv\")\n",
    "\n",
    "# Charger le dataset\n",
    "dataset = load_dataset(\"Tirendaz/fifa-world-cup-2022-tweets\")\n",
    "\n",
    "tweets = dataset['train']['Tweet']\n",
    "sentiments = dataset['train']['Sentiment']\n",
    "# Créer un DataFrame avec les deux listes\n",
    "dataset = pd.DataFrame({\n",
    "    'Tweet': tweets,\n",
    "    'Sentiment': sentiments\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cypri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                           drinking today\n",
      "1        amazing launch video shows much face canada me...\n",
      "2                                   worth reading watching\n",
      "3                            golden maknae shinning bright\n",
      "4        bbc cares much human rights homosexual rights ...\n",
      "                               ...                        \n",
      "22519                                    go world cup 2022\n",
      "22520    anderlecht confirms former viborg ffs jesper f...\n",
      "22521                              great thread read start\n",
      "22522                         raphinha wants brazil united\n",
      "22523    buy sot pinksale confused buy tokens pinksale ...\n",
      "Name: Tweet, Length: 22524, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Télécharger les stopwords de NLTK si nécessaire\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Fonction pour prétraiter les tweets\n",
    "def preprocess_tweet(tweet):\n",
    "    # Remplacer les retours à la ligne par des espaces\n",
    "    tweet = re.sub(r'\\n', ' ', tweet)\n",
    "    # Remplacer les retours chariots par des espaces\n",
    "    tweet = re.sub(r'\\r', ' ', tweet)\n",
    "    # Enlever les URLs\n",
    "    tweet = re.sub(r'http\\S+|www\\S+', '', tweet)\n",
    "    # Enlever les mentions\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    # Enlever les hashtags\n",
    "    tweet = re.sub(r'#\\w+', '', tweet)\n",
    "    # Enlever les caractères spéciaux en gardant seulement les alphanumériques et les espaces\n",
    "    tweet = ''.join([char for char in tweet if char.isalnum() or char.isspace()])\n",
    "    # Convertir en minuscules\n",
    "    tweet = tweet.lower()\n",
    "    # Enlever les stopwords\n",
    "    tweet = ' '.join([word for word in tweet.split() if word not in stop_words])\n",
    "    return tweet\n",
    "\n",
    "# Appliquer le prétraitement à chaque tweet\n",
    "tweets_df = dataset['Tweet'].apply(preprocess_tweet)\n",
    "\n",
    "# Afficher les tweets prétraités\n",
    "print(tweets_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation et Préparation des Séquences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nous utilisons Tokenizer de Keras pour transformer les tweets en séquences de tokens. La taille du vocabulaire est augmentée à 10000 mots.\n",
    "* Les séquences sont ensuite remplies (padding) à une longueur fixe de 120 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation : Transformer les tweets en séquences de tokens\n",
    "tokenizer = Tokenizer(num_words=10000)  \n",
    "tokenizer.fit_on_texts(tweets_df)\n",
    "X = tokenizer.texts_to_sequences(tweets_df)\n",
    "\n",
    "# Padding des séquences : Normaliser les séquences à une longueur fixe\n",
    "X = pad_sequences(X, maxlen=60)  # Augmentation de la longueur maximale des séquences\n",
    "\n",
    "# Convertir les labels de sentiment en valeurs numériques\n",
    "dataset['Sentiment'] = dataset['Sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
    "y = dataset['Sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(dataset['Sentiment'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition et Entraînement du Modèle LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nous définissons un modèle LSTM bidirectionnel avec des couches de dropout pour régulariser les données et éviter le surapprentissage.\n",
    "* Le modèle est compilé avec la perte sparse_categorical_crossentropy et l'optimiseur Adam.\n",
    "* Nous entraînons le modèle sur les données d'entraînement et validons sur les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Définition de la classe pour le modèle LSTM personnalisé\n",
    "class CustomLSTMModel:\n",
    "    def __init__(self, vocab_size, embedding_dim, input_length, hidden_dim, output_size):\n",
    "        # Initialiser le modèle séquentiel\n",
    "        self.model = Sequential()\n",
    "        # Ajouter une couche d'embedding\n",
    "        self.model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length))\n",
    "        # Ajouter un SpatialDropout pour régulariser les embeddings\n",
    "        self.model.add(SpatialDropout1D(0.3)) \n",
    "        # Ajouter une couche LSTM bidirectionnelle avec dropout\n",
    "        self.model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)))\n",
    "        # Ajouter une couche LSTM avec dropout\n",
    "        self.model.add(LSTM(hidden_dim, dropout=0.3, recurrent_dropout=0.3))\n",
    "        # Ajouter une couche dense avec activation softmax pour la classification\n",
    "        self.model.add(Dense(output_size, activation='softmax'))\n",
    "        \n",
    "        # Compiler le modèle avec la perte 'sparse_categorical_crossentropy' et l'optimiseur Adam\n",
    "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "    # Fonction pour entraîner le modèle\n",
    "    def fit(self, X_train, y_train, validation_data, epochs=10, batch_size=64):  # Augmentation des époques\n",
    "        history = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data, verbose=1)\n",
    "        return history\n",
    "\n",
    "    # Fonction pour évaluer le modèle\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        loss, accuracy = self.model.evaluate(X_test, y_test, verbose=1)\n",
    "        return loss, accuracy\n",
    "\n",
    "    # Fonction pour prédire les classes des nouvelles données\n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test)\n",
    "\n",
    "# Assurer que les données sont sous forme de tableau numpy\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialiser et entraîner le modèle LSTM personnalisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1127/1127 [==============================] - 78s 62ms/step - loss: 0.8102 - accuracy: 0.6170 - val_loss: 0.6774 - val_accuracy: 0.7070\n",
      "Epoch 2/4\n",
      "1127/1127 [==============================] - 74s 66ms/step - loss: 0.5661 - accuracy: 0.7646 - val_loss: 0.6556 - val_accuracy: 0.7154\n",
      "Epoch 3/4\n",
      "1127/1127 [==============================] - 73s 65ms/step - loss: 0.4729 - accuracy: 0.8099 - val_loss: 0.7039 - val_accuracy: 0.7103\n",
      "Epoch 4/4\n",
      "1127/1127 [==============================] - 71s 63ms/step - loss: 0.4178 - accuracy: 0.8315 - val_loss: 0.7323 - val_accuracy: 0.7072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24db6d28990>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialiser et entraîner le modèle LSTM personnalisé\n",
    "\n",
    "# Taille du vocabulaire ajustée\n",
    "vocab_size = 10000 \n",
    "\n",
    "# Dimension des embeddings \n",
    "embedding_dim = 50 #128 #50 \n",
    "\n",
    "# Longueur maximale des séquences\n",
    "input_length = 60 #120 #60\n",
    "\n",
    "# Nombre de neurones dans la couche cachée LSTM\n",
    "hidden_dim = 32 #128 #32\n",
    "\n",
    "# Pour trois classes : négatif, neutre, positif\n",
    "output_size = 3  \n",
    "\n",
    "# Nombre d'époques d'entraînement\n",
    "epochs= 4 #10 #4\n",
    "\n",
    "# Taille des lots\n",
    "batch_size = 16 #64 #16\n",
    "\n",
    "\n",
    "custom_lstm_model = CustomLSTMModel(vocab_size, embedding_dim, input_length, hidden_dim, output_size)\n",
    "custom_lstm_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interprétation :\n",
    "\n",
    "* La perte d'entraînement diminue bien, atteignant 0.4187, et l'exactitude d'entraînement augmente à 83.17%.\n",
    "* La perte de validation continue d'augmenter légèrement, atteignant 0.7109, ce qui renforce l'indication de surapprentissage.\n",
    "* L'exactitude de validation reste stable à 70.74%, indiquant que malgré l'augmentation de la performance sur les données d'entraînement, le modèle ne s'améliore pas sur les données de validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluation et Prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nous évaluons le modèle sur les données de test pour obtenir la perte et la précision.\n",
    "* Nous prédisons les sentiments des tweets de test et affichons un rapport de classification pour évaluer les performances du modèle.\n",
    "* Pour comparaison, nous entraînons également un modèle de régression logistique et comparons ses performances avec le modèle LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 2s 11ms/step - loss: 0.7323 - accuracy: 0.7072\n",
      "Test Loss: 0.7323295474052429 | Test Accuracy: 0.7072141766548157\n",
      "141/141 [==============================] - 2s 11ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.72      0.72      1149\n",
      "     neutral       0.67      0.62      0.65      1648\n",
      "    positive       0.72      0.78      0.75      1708\n",
      "\n",
      "    accuracy                           0.71      4505\n",
      "   macro avg       0.71      0.71      0.71      4505\n",
      "weighted avg       0.71      0.71      0.71      4505\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "Logistic Regression Accuracy: 0.3908990011098779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.27      0.03      0.05      1149\n",
      "     neutral       0.37      0.36      0.37      1648\n",
      "    positive       0.41      0.67      0.50      1708\n",
      "\n",
      "    accuracy                           0.39      4505\n",
      "   macro avg       0.35      0.35      0.31      4505\n",
      "weighted avg       0.36      0.39      0.34      4505\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cypri\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Évaluer le modèle\n",
    "loss, accuracy = custom_lstm_model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss} | Test Accuracy: {accuracy}')\n",
    "\n",
    "# Prédire et imprimer le rapport de classification\n",
    "y_pred = np.argmax(custom_lstm_model.predict(X_test), axis=-1)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['negative', 'neutral', 'positive']))\n",
    "\n",
    "# Modèle de régression logistique pour comparaison\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Entraîner le modèle de régression logistique\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire et évaluer\n",
    "y_pred_logistic = logistic_model.predict(X_test)\n",
    "print(\"----------------------------------------------------------------------------\")\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_logistic))\n",
    "print(classification_report(y_test, y_pred_logistic, target_names=['negative', 'neutral', 'positive']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interprétation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Modèle LSTM</u>:\n",
    "\n",
    "* L'exactitude de test de 70.74% est cohérente avec l'exactitude de validation observée pendant l'entraînement, suggérant que le modèle généralise bien aux données de test.\n",
    "  * Test Loss (Perte de Test) : 71.09%\n",
    "  * Test Accuracy (Exactitude de Test) : 70.74%\n",
    "\n",
    "* La classe \"positive\" a la meilleure précision, tandis que la classe \"neutral\" a le meilleur rappel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Régression logistique</u>:\n",
    "\n",
    "* L'exactitude du modèle de régression logistique est bien inférieure à celle du modèle LSTM, indiquant que la régression logistique a du mal à capturer les relations complexes dans les données de tweets.\n",
    "\n",
    "* La performance de la régression logistique est particulièrement faible pour la classe \"negative\", avec un rappel de seulement 3%. Cela suggère que le modèle de régression logistique ne parvient pas à capturer les caractéristiques des tweets négatifs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "L'exactitude du modèle de régression logistique est bien inférieure à celle du modèle LSTM, indiquant que la régression logistique a du mal à capturer les relations complexes dans les données de tweets.\n",
    "* Test Accuracy Régression Logistique : 39.09%\n",
    "* Test Accuracy Modèle LSTM : 70.74%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>ConvergenceWarning </u>:\n",
    "\n",
    "L'avertissement indique que l'optimiseur L-BFGS a atteint le nombre maximum d'itérations avant de converger. Pour résoudre ce problème, il est possible :\n",
    "\n",
    "* D'augmenter le nombre d'itérations (max_iter).\n",
    "* De prétraiter les données en les normalisant ou en les standardisant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
