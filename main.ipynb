{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.0.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: transformers==4.30.2 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (4.30.2)\n",
      "Requirement already satisfied: datasets==2.20.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (2.20.0)\n",
      "Requirement already satisfied: pandas==1.5.3 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (1.5.3)\n",
      "Requirement already satisfied: nltk==3.8.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (3.8.1)\n",
      "Requirement already satisfied: scikit-learn==1.2.2 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 7)) (1.2.2)\n",
      "Requirement already satisfied: tensorflow==2.12.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 8)) (2.12.0)\n",
      "Requirement already satisfied: pyarrow==16.1.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 9)) (16.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.23.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (4.66.4)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0->-r requirements.txt (line 3)) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (3.9.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from pandas==1.5.3->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from pandas==1.5.3->-r requirements.txt (line 4)) (2023.3.post1)\n",
      "Requirement already satisfied: click in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from nltk==3.8.1->-r requirements.txt (line 5)) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from nltk==3.8.1->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 7)) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 7)) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow==2.12.0->-r requirements.txt (line 8)) (2.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (3.9.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.4.29)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.4.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.12.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.31.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (1.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.30.2->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from jinja2->torch==2.0.1->-r requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from sympy->torch==2.0.1->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.41.2)\n",
      "Requirement already satisfied: ml-dtypes>=0.4.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.4.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.30.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.0.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prétraitement des Données\n",
    "\n",
    "* Nous commençons par charger les données depuis un fichier CSV et afficher les informations sur les sentiments pour comprendre la structure des données.\n",
    "* Ensuite, nous définissons une fonction de prétraitement pour nettoyer les tweets en enlevant les caractères spéciaux, les URLs, les mentions et les hashtags, et en supprimant les stopwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         neutral\n",
      "1        positive\n",
      "2        positive\n",
      "3        positive\n",
      "4        negative\n",
      "           ...   \n",
      "22519    positive\n",
      "22520     neutral\n",
      "22521    positive\n",
      "22522    positive\n",
      "22523     neutral\n",
      "Name: Sentiment, Length: 22524, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cypri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                           drinking today\n",
      "1        amazing launch video shows much face canada me...\n",
      "2                                   worth reading watching\n",
      "3                            golden maknae shinning bright\n",
      "4        bbc cares much human rights homosexual rights ...\n",
      "                               ...                        \n",
      "22519                                    go world cup 2022\n",
      "22520    anderlecht confirms former viborg ffs jesper f...\n",
      "22521                              great thread read start\n",
      "22522                         raphinha wants brazil united\n",
      "22523    buy sot pinksale confused buy tokens pinksale ...\n",
      "Name: Tweet, Length: 22524, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#dataset = pd.read_csv(\"C:/Users/flavi/Downloads/fifa_world_cup_2022_tweets.csv\")\n",
    "\n",
    "# Charger le dataset\n",
    "dataset = load_dataset(\"Tirendaz/fifa-world-cup-2022-tweets\")\n",
    "\n",
    "tweets = dataset['train']['Tweet']\n",
    "sentiments = dataset['train']['Sentiment']\n",
    "\n",
    "# Créer un DataFrame avec les deux listes\n",
    "dataset = pd.DataFrame({\n",
    "    'Tweet': tweets,\n",
    "    'Sentiment': sentiments\n",
    "})\n",
    "# Afficher des informations sur le dataset pour voir sa structure\n",
    "print(dataset['Sentiment'])\n",
    "\n",
    "# Télécharger les stopwords de NLTK si nécessaire\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Fonction pour prétraiter les tweets\n",
    "def preprocess_tweet(tweet):\n",
    "    # Remplacer les retours à la ligne par des espaces\n",
    "    tweet = re.sub(r'\\n', ' ', tweet)\n",
    "    # Remplacer les retours chariots par des espaces\n",
    "    tweet = re.sub(r'\\r', ' ', tweet)\n",
    "    # Enlever les URLs\n",
    "    tweet = re.sub(r'http\\S+|www\\S+', '', tweet)\n",
    "    # Enlever les mentions\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    # Enlever les hashtags\n",
    "    tweet = re.sub(r'#\\w+', '', tweet)\n",
    "    # Enlever les caractères spéciaux en gardant seulement les alphanumériques et les espaces\n",
    "    tweet = ''.join([char for char in tweet if char.isalnum() or char.isspace()])\n",
    "    # Convertir en minuscules\n",
    "    tweet = tweet.lower()\n",
    "    # Enlever les stopwords\n",
    "    tweet = ' '.join([word for word in tweet.split() if word not in stop_words])\n",
    "    return tweet\n",
    "\n",
    "# Appliquer le prétraitement à chaque tweet\n",
    "tweets_df = dataset['Tweet'].apply(preprocess_tweet)\n",
    "\n",
    "# Afficher les tweets prétraités\n",
    "print(tweets_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation et Préparation des Séquences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nous utilisons Tokenizer de Keras pour transformer les tweets en séquences de tokens. La taille du vocabulaire est augmentée à 10000 mots.\n",
    "* Les séquences sont ensuite remplies (padding) à une longueur fixe de 120 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation : Transformer les tweets en séquences de tokens\n",
    "tokenizer = Tokenizer(num_words=10000)  \n",
    "tokenizer.fit_on_texts(tweets_df)\n",
    "X = tokenizer.texts_to_sequences(tweets_df)\n",
    "\n",
    "# Padding des séquences : Normaliser les séquences à une longueur fixe\n",
    "X = pad_sequences(X, maxlen=120)  # Augmentation de la longueur maximale des séquences\n",
    "\n",
    "# Convertir les labels de sentiment en valeurs numériques\n",
    "dataset['Sentiment'] = dataset['Sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
    "y = dataset['Sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition et Entraînement du Modèle LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nous définissons un modèle LSTM bidirectionnel avec des couches de dropout pour régulariser les données et éviter le surapprentissage.\n",
    "* Le modèle est compilé avec la perte sparse_categorical_crossentropy et l'optimiseur Adam.\n",
    "* Nous entraînons le modèle sur les données d'entraînement et validons sur les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Définition de la classe pour le modèle LSTM personnalisé\n",
    "class CustomLSTMModel:\n",
    "    def __init__(self, vocab_size, embedding_dim, input_length, hidden_dim, output_size):\n",
    "        # Initialiser le modèle séquentiel\n",
    "        self.model = Sequential()\n",
    "        # Ajouter une couche d'embedding\n",
    "        self.model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length))\n",
    "        # Ajouter un SpatialDropout pour régulariser les embeddings\n",
    "        self.model.add(SpatialDropout1D(0.3)) \n",
    "        # Ajouter une couche LSTM bidirectionnelle avec dropout\n",
    "        self.model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)))\n",
    "        # Ajouter une couche LSTM avec dropout\n",
    "        self.model.add(LSTM(hidden_dim, dropout=0.3, recurrent_dropout=0.3))\n",
    "        # Ajouter une couche dense avec activation softmax pour la classification\n",
    "        self.model.add(Dense(output_size, activation='softmax'))\n",
    "        \n",
    "        # Compiler le modèle avec la perte 'sparse_categorical_crossentropy' et l'optimiseur Adam\n",
    "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "    # Fonction pour entraîner le modèle\n",
    "    def fit(self, X_train, y_train, validation_data, epochs=10, batch_size=64):  # Augmentation des époques\n",
    "        history = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data, verbose=1)\n",
    "        return history\n",
    "\n",
    "    # Fonction pour évaluer le modèle\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        loss, accuracy = self.model.evaluate(X_test, y_test, verbose=1)\n",
    "        return loss, accuracy\n",
    "\n",
    "    # Fonction pour prédire les classes des nouvelles données\n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test)\n",
    "\n",
    "# Assurer que les données sont sous forme de tableau numpy\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialiser et entraîner le modèle LSTM personnalisé\n",
    "vocab_size = 10000  # Taille du vocabulaire ajustée\n",
    "embedding_dim = 128\n",
    "input_length = 120\n",
    "hidden_dim = 128  # Augmentation de la dimension cachée\n",
    "output_size = 3  # Pour trois classes : négatif, neutre, positif\n",
    "\n",
    "custom_lstm_model = CustomLSTMModel(vocab_size, embedding_dim, input_length, hidden_dim, output_size)\n",
    "custom_lstm_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluation et Prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nous évaluons le modèle sur les données de test pour obtenir la perte et la précision.\n",
    "* Nous prédisons les sentiments des tweets de test et affichons un rapport de classification pour évaluer les performances du modèle.\n",
    "* Pour comparaison, nous entraînons également un modèle de régression logistique et comparons ses performances avec le modèle LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Évaluer le modèle\n",
    "loss, accuracy = custom_lstm_model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss} | Test Accuracy: {accuracy}')\n",
    "\n",
    "# Prédire et imprimer le rapport de classification\n",
    "y_pred = np.argmax(custom_lstm_model.predict(X_test), axis=-1)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['negative', 'neutral', 'positive']))\n",
    "\n",
    "# Modèle de régression logistique pour comparaison\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Entraîner le modèle de régression logistique\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire et évaluer\n",
    "y_pred_logistic = logistic_model.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_logistic))\n",
    "print(classification_report(y_test, y_pred_logistic, target_names=['negative', 'neutral', 'positive']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
