{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.0.1 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: transformers==4.30.2 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (4.30.2)\n",
      "Requirement already satisfied: datasets==2.20.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (2.20.0)\n",
      "Requirement already satisfied: pandas==1.5.3 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (1.5.3)\n",
      "Requirement already satisfied: nltk==3.8.1 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (3.8.1)\n",
      "Requirement already satisfied: scikit-learn==1.2.2 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 7)) (1.2.2)\n",
      "Requirement already satisfied: tensorflow==2.12.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 8)) (2.12.0)\n",
      "Requirement already satisfied: pyarrow==16.1.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 9)) (16.1.0)\n",
      "Requirement already satisfied: numpy==1.23.5 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 10)) (1.23.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.23.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (4.66.4)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0->-r requirements.txt (line 3)) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (3.9.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from pandas==1.5.3->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from pandas==1.5.3->-r requirements.txt (line 4)) (2023.3.post1)\n",
      "Requirement already satisfied: click in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from nltk==3.8.1->-r requirements.txt (line 5)) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from nltk==3.8.1->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 7)) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 7)) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorflow==2.12.0->-r requirements.txt (line 8)) (2.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (3.11.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.4.29)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.4.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.12.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.31.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (1.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.30.2->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from jinja2->torch==2.0.1->-r requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from sympy->torch==2.0.1->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.41.2)\n",
      "Requirement already satisfied: ml-dtypes>=0.4.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.4.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.30.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.0.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing packages used in the code\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretreatment of the data\n",
    "\n",
    "* We start by loading the data from a CSV and display the informations of the dataset to understand its structure. \n",
    "* Then, we define a function of pretreatment to clean tweets by removing special characters, URLs, mentions, tagging and hashtags, and deleting the stopwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0               Date Created  Number of Likes  \\\n",
      "0           0  2022-11-20 23:59:21+00:00                4   \n",
      "1           1  2022-11-20 23:59:01+00:00                3   \n",
      "2           2  2022-11-20 23:58:41+00:00                1   \n",
      "3           3  2022-11-20 23:58:33+00:00                1   \n",
      "4           4  2022-11-20 23:58:28+00:00                0   \n",
      "\n",
      "       Source of Tweet                                              Tweet  \\\n",
      "0      Twitter Web App  What are we drinking today @TucanTribe \\n@MadB...   \n",
      "1   Twitter for iPhone  Amazing @CanadaSoccerEN  #WorldCup2022 launch ...   \n",
      "2   Twitter for iPhone  Worth reading while watching #WorldCup2022 htt...   \n",
      "3      Twitter Web App  Golden Maknae shinning bright\\n\\nhttps://t.co/...   \n",
      "4  Twitter for Android  If the BBC cares so much about human rights, h...   \n",
      "\n",
      "  Sentiment  \n",
      "0   neutral  \n",
      "1  positive  \n",
      "2  positive  \n",
      "3  positive  \n",
      "4  negative  \n"
     ]
    }
   ],
   "source": [
    "# Importing data\n",
    "current_dir = os.getcwd()\n",
    "dataset_name = \"fifa_world_cup_2022_tweets.csv\"\n",
    "file_path = os.path.join(current_dir, dataset_name)\n",
    "\n",
    "dataset = pd.read_csv(file_path)\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are we drinking today @TucanTribe \\n@MadB...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazing @CanadaSoccerEN  #WorldCup2022 launch ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Worth reading while watching #WorldCup2022 htt...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Golden Maknae shinning bright\\n\\nhttps://t.co/...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If the BBC cares so much about human rights, h...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22519</th>\n",
       "      <td>Here We go World cup 2022 #WorldCup2022</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22520</th>\n",
       "      <td>Anderlecht confirms former Viborg FF's Jesper ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22521</th>\n",
       "      <td>Great thread to read before the start of #Worl...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22522</th>\n",
       "      <td>Raphinha wants Brazil to be united at the #Wor...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22523</th>\n",
       "      <td>How to buy $SOT on PinkSale?🤔\\n\\nHave you been...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22524 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Tweet Sentiment\n",
       "0      What are we drinking today @TucanTribe \\n@MadB...   neutral\n",
       "1      Amazing @CanadaSoccerEN  #WorldCup2022 launch ...  positive\n",
       "2      Worth reading while watching #WorldCup2022 htt...  positive\n",
       "3      Golden Maknae shinning bright\\n\\nhttps://t.co/...  positive\n",
       "4      If the BBC cares so much about human rights, h...  negative\n",
       "...                                                  ...       ...\n",
       "22519            Here We go World cup 2022 #WorldCup2022  positive\n",
       "22520  Anderlecht confirms former Viborg FF's Jesper ...   neutral\n",
       "22521  Great thread to read before the start of #Worl...  positive\n",
       "22522  Raphinha wants Brazil to be united at the #Wor...  positive\n",
       "22523  How to buy $SOT on PinkSale?🤔\\n\\nHave you been...   neutral\n",
       "\n",
       "[22524 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keeping the tweets and sentiment columns only\n",
    "dataset = dataset[['Tweet','Sentiment']]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\flavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                           drinking today\n",
      "1        amazing launch video shows much face canada me...\n",
      "2                                   worth reading watching\n",
      "3                            golden maknae shinning bright\n",
      "4        bbc cares much human rights homosexual rights ...\n",
      "                               ...                        \n",
      "22519                                    go world cup 2022\n",
      "22520    anderlecht confirms former viborg ffs jesper f...\n",
      "22521                              great thread read start\n",
      "22522                         raphinha wants brazil united\n",
      "22523    buy sot pinksale confused buy tokens pinksale ...\n",
      "Name: Tweet, Length: 22524, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Downloading the stopwords from NLTK \n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess and clean the tweets\n",
    "def preprocess_tweet(tweet):\n",
    "    # Replacing the linebreaks by spaces\n",
    "    tweet = re.sub(r'\\n', ' ', tweet)\n",
    "    # Replace carriage returns with spaces\n",
    "    tweet = re.sub(r'\\r', ' ', tweet)\n",
    "    # Removing urls\n",
    "    tweet = re.sub(r'http\\S+|www\\S+', '', tweet)\n",
    "    # Removing mentions (@)\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    # Removing hashtags\n",
    "    tweet = re.sub(r'#\\w+', '', tweet)\n",
    "    # Removing special characters and keeping only the alphanumericals and spaces\n",
    "    tweet = ''.join([char for char in tweet if char.isalnum() or char.isspace()])\n",
    "    # Convert the tweets in lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # Removing stopwords\n",
    "    tweet = ' '.join([word for word in tweet.split() if word not in stop_words])\n",
    "    return tweet\n",
    "\n",
    "# Apply pretreatment to each tweet\n",
    "tweets_df = dataset['Tweet'].apply(preprocess_tweet)\n",
    "\n",
    "# Print pretreated tweets\n",
    "print(tweets_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and preparation of sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We use Tokenizer from Keras to transform our tweets into sequences of tokens. The size of the vocabulary is increased to 1000 words. \n",
    "* The sequences are then filled (padding) with a fixed length of 120 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 ... 2 2 1]\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation : Transforming the tweets into sequences of tokens\n",
    "tokenizer = Tokenizer(num_words=10000)  \n",
    "tokenizer.fit_on_texts(tweets_df)\n",
    "sequences = tokenizer.texts_to_sequences(tweets_df)\n",
    "\n",
    "# Padding sequences : Normalizing the sequences to a fixed length of 60\n",
    "padded_sequences = pad_sequences(sequences, maxlen=60)  # Increases the max length of sequences\n",
    "\n",
    "# Converting sentiment labels into numerical values\n",
    "dataset['Sentiment'] = dataset['Sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
    "y = dataset['Sentiment'].values\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(dataset['Sentiment'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition and training of the LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We define a LSTM bidirectional model with dropout layers to adjust the data and avoid overfitting.\n",
    "* The model is compiled with the loss sparse_categorical_crossentropy and optimizer Adam.\n",
    "* We train our model on the train data and then validate the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for the customized LSTM model\n",
    "class CustomLSTMModel:\n",
    "    def __init__(self, vocab_size, embedding_dim, input_length, hidden_dim, output_size):\n",
    "        # Initializing the sequential model\n",
    "        self.model = Sequential()\n",
    "        # Adding a layer of embedding\n",
    "        self.model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length))\n",
    "        # Adding a SpatialDropout to regularize embeddings\n",
    "        self.model.add(SpatialDropout1D(0.3)) \n",
    "        # Adding a layer of bidirectional LSTM with dropout\n",
    "        self.model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)))\n",
    "        # Adding a layer of LSTM with dropout\n",
    "        self.model.add(LSTM(hidden_dim, dropout=0.3, recurrent_dropout=0.3))\n",
    "        # Adding a dense layer with softmax activation for classification\n",
    "        self.model.add(Dense(output_size, activation='softmax'))\n",
    "        \n",
    "        # Compiling the model with the loss 'sparse_categorical_crossentropy' and optimizer Adam\n",
    "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "    # Function to train the model \n",
    "    def fit(self, X_train, y_train, validation_data, epochs=10, batch_size=64): \n",
    "        history = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data, verbose=1)\n",
    "        return history\n",
    "\n",
    "    # Function to evaluate the model\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        loss, accuracy = self.model.evaluate(X_test, y_test, verbose=1)\n",
    "        return loss, accuracy\n",
    "\n",
    "    # Function to predict classes of the new data\n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test)\n",
    "\n",
    "# Making sure all the data are numpy arrays\n",
    "X = np.array(padded_sequences)\n",
    "y = np.array(y)\n",
    "\n",
    "# Splitting the data into training data and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1127/1127 [==============================] - 100s 78ms/step - loss: 0.8222 - accuracy: 0.6072 - val_loss: 0.6717 - val_accuracy: 0.7034\n",
      "Epoch 2/4\n",
      "1127/1127 [==============================] - 186s 165ms/step - loss: 0.5670 - accuracy: 0.7635 - val_loss: 0.6527 - val_accuracy: 0.7203\n",
      "Epoch 3/4\n",
      "1127/1127 [==============================] - 213s 189ms/step - loss: 0.4713 - accuracy: 0.8098 - val_loss: 0.6917 - val_accuracy: 0.7085\n",
      "Epoch 4/4\n",
      "1127/1127 [==============================] - 210s 186ms/step - loss: 0.4151 - accuracy: 0.8355 - val_loss: 0.7056 - val_accuracy: 0.7072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a7e278ff90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing and training the custom LSTM model\n",
    "\n",
    "# Size of vocab adjusted\n",
    "vocab_size = 10000 \n",
    "\n",
    "# Dimension of embeddings \n",
    "embedding_dim = 50 #128 #50 \n",
    "\n",
    "# Max length for sequences\n",
    "input_length = 60 #120 #60\n",
    "\n",
    "# Number of neurones in the hidden LSTM layer\n",
    "hidden_dim = 32 #128 #32\n",
    "\n",
    "# For 3 classes : negative, neutral, positive\n",
    "output_size = 3  \n",
    "\n",
    "# Number of epochs for training\n",
    "epochs= 4 #10 #4\n",
    "\n",
    "# Size of batches\n",
    "batch_size = 16 #64 #16\n",
    "\n",
    "\n",
    "custom_lstm_model = CustomLSTMModel(vocab_size, embedding_dim, input_length, hidden_dim, output_size)\n",
    "custom_lstm_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation :\n",
    "\n",
    "* The loss in training decreases well, reaching 0.4151, and the accuracy of training increases to 83.39%.\n",
    "* The loss in validation keeps rising slighlty, reaching 0.7056, which reinforces the overfitting indicator.\n",
    "* The validation accuracy stays stable at around 70.72%, indicating that despite the increase in performance on the training data, the model does not significatively improve on the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We evaluate the model on the test data to obtain the loss and precision.\n",
    "* We predict the sentiments in the test tweets and display a classification report in order to evaluate the model performance.\n",
    "* For comparaison, we also train a logistic regression model and compare its performances with our custom LSTM model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 4s 28ms/step - loss: 0.7056 - accuracy: 0.7072\n",
      "Test Loss: 0.7056096792221069 | Test Accuracy: 0.7072141766548157\n",
      "141/141 [==============================] - 5s 32ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.70      0.71      1149\n",
      "     neutral       0.65      0.68      0.67      1648\n",
      "    positive       0.75      0.74      0.74      1708\n",
      "\n",
      "    accuracy                           0.71      4505\n",
      "   macro avg       0.71      0.71      0.71      4505\n",
      "weighted avg       0.71      0.71      0.71      4505\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "Logistic Regression Accuracy: 0.3849056603773585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.24      0.03      0.05      1149\n",
      "     neutral       0.37      0.28      0.32      1648\n",
      "    positive       0.40      0.73      0.51      1708\n",
      "\n",
      "    accuracy                           0.38      4505\n",
      "   macro avg       0.34      0.34      0.29      4505\n",
      "weighted avg       0.35      0.38      0.32      4505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "loss, accuracy = custom_lstm_model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss} | Test Accuracy: {accuracy}')\n",
    "\n",
    "# Prediction and displaying the classification report\n",
    "y_pred = np.argmax(custom_lstm_model.predict(X_test), axis=-1)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['negative', 'neutral', 'positive']))\n",
    "\n",
    "# Logistic regression model to use as comparison\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(X_train)\n",
    "x_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Training the logistic regression model\n",
    "logistic_model = LogisticRegression(max_iter=500)\n",
    "logistic_model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Predicting and evaluating \n",
    "y_pred_logistic = logistic_model.predict(x_test_scaled)\n",
    "print(\"----------------------------------------------------------------------------\")\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_logistic))\n",
    "print(classification_report(y_test, y_pred_logistic, target_names=['negative', 'neutral', 'positive']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>LSTM model</u>:\n",
    "\n",
    "* The accuracy of the test of 70.72% is coherent with the validation accuracy observed during training, suggesting that the model generalizes well our test data.\n",
    "  * Test Loss : 70.56%\n",
    "  * Test Accuracy : 70.72%\n",
    "\n",
    "* The 'positive' class seems to be more precise overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Logistic regression model</u>:\n",
    "\n",
    "* The accuracy of the logistic regression model is inferior to the accuracy of the LSTM model, indicating that the logistic regression seems to struggle capturing the complex relationships between the tweets and sentiments.\n",
    "\n",
    "* The performance of the logistic regression model is especially poor for the 'negative' class, with a recall of only 3%. These results suggest that the logistic regression model cannot capture the specificities of the negative tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The accuracy of the linear regression model is extremely inferior to the accuracy of the custom LSTM model, indicating again that the logistic regression struggles to capture the relation between the tweets and sentiments.\n",
    "* Test Accuracy Logistic Regression: 38.49%\n",
    "* Test Accuracy LSTM model: 70.72%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
