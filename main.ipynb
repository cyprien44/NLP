{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.0.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: transformers==4.30.2 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (4.30.2)\n",
      "Requirement already satisfied: datasets==2.20.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (2.20.0)\n",
      "Requirement already satisfied: pandas==1.5.3 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (1.5.3)\n",
      "Requirement already satisfied: nltk==3.8.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (3.8.1)\n",
      "Requirement already satisfied: scikit-learn==1.2.2 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 7)) (1.2.2)\n",
      "Requirement already satisfied: tensorflow==2.12.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 8)) (2.12.0)\n",
      "Requirement already satisfied: pyarrow==16.1.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 9)) (16.1.0)\n",
      "Requirement already satisfied: numpy==1.23.5 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 10)) (1.23.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.23.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (4.66.4)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0->-r requirements.txt (line 3)) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (3.9.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from pandas==1.5.3->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from pandas==1.5.3->-r requirements.txt (line 4)) (2023.3.post1)\n",
      "Requirement already satisfied: click in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from nltk==3.8.1->-r requirements.txt (line 5)) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from nltk==3.8.1->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 7)) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 7)) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow==2.12.0->-r requirements.txt (line 8)) (2.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (3.9.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.4.29)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.4.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.12.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.31.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (1.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.30.2->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from jinja2->torch==2.0.1->-r requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from sympy->torch==2.0.1->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.41.2)\n",
      "Requirement already satisfied: ml-dtypes>=0.4.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.4.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.30.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (2.0.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\cypri\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0->-r requirements.txt (line 8)) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pr√©traitement des Donn√©es\n",
    "\n",
    "* Nous commen√ßons par charger les donn√©es depuis un fichier CSV et afficher les informations sur les sentiments pour comprendre la structure des donn√©es.\n",
    "* Ensuite, nous d√©finissons une fonction de pr√©traitement pour nettoyer les tweets en enlevant les caract√®res sp√©ciaux, les URLs, les mentions et les hashtags, et en supprimant les stopwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are we drinking today @TucanTribe \\n@MadB...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazing @CanadaSoccerEN  #WorldCup2022 launch ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Worth reading while watching #WorldCup2022 htt...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Golden Maknae shinning bright\\n\\nhttps://t.co/...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If the BBC cares so much about human rights, h...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22519</th>\n",
       "      <td>Here We go World cup 2022 #WorldCup2022</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22520</th>\n",
       "      <td>Anderlecht confirms former Viborg FF's Jesper ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22521</th>\n",
       "      <td>Great thread to read before the start of #Worl...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22522</th>\n",
       "      <td>Raphinha wants Brazil to be united at the #Wor...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22523</th>\n",
       "      <td>How to buy $SOT on PinkSale?ü§î\\n\\nHave you been...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22524 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Tweet Sentiment\n",
       "0      What are we drinking today @TucanTribe \\n@MadB...   neutral\n",
       "1      Amazing @CanadaSoccerEN  #WorldCup2022 launch ...  positive\n",
       "2      Worth reading while watching #WorldCup2022 htt...  positive\n",
       "3      Golden Maknae shinning bright\\n\\nhttps://t.co/...  positive\n",
       "4      If the BBC cares so much about human rights, h...  negative\n",
       "...                                                  ...       ...\n",
       "22519            Here We go World cup 2022 #WorldCup2022  positive\n",
       "22520  Anderlecht confirms former Viborg FF's Jesper ...   neutral\n",
       "22521  Great thread to read before the start of #Worl...  positive\n",
       "22522  Raphinha wants Brazil to be united at the #Wor...  positive\n",
       "22523  How to buy $SOT on PinkSale?ü§î\\n\\nHave you been...   neutral\n",
       "\n",
       "[22524 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#dataset = pd.read_csv(\"C:/Users/flavi/Downloads/fifa_world_cup_2022_tweets.csv\")\n",
    "\n",
    "# Charger le dataset\n",
    "dataset = load_dataset(\"Tirendaz/fifa-world-cup-2022-tweets\")\n",
    "\n",
    "tweets = dataset['train']['Tweet']\n",
    "sentiments = dataset['train']['Sentiment']\n",
    "# Cr√©er un DataFrame avec les deux listes\n",
    "dataset = pd.DataFrame({\n",
    "    'Tweet': tweets,\n",
    "    'Sentiment': sentiments\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cypri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                           drinking today\n",
      "1        amazing launch video shows much face canada me...\n",
      "2                                   worth reading watching\n",
      "3                            golden maknae shinning bright\n",
      "4        bbc cares much human rights homosexual rights ...\n",
      "                               ...                        \n",
      "22519                                    go world cup 2022\n",
      "22520    anderlecht confirms former viborg ffs jesper f...\n",
      "22521                              great thread read start\n",
      "22522                         raphinha wants brazil united\n",
      "22523    buy sot pinksale confused buy tokens pinksale ...\n",
      "Name: Tweet, Length: 22524, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# T√©l√©charger les stopwords de NLTK si n√©cessaire\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Fonction pour pr√©traiter les tweets\n",
    "def preprocess_tweet(tweet):\n",
    "    # Remplacer les retours √† la ligne par des espaces\n",
    "    tweet = re.sub(r'\\n', ' ', tweet)\n",
    "    # Remplacer les retours chariots par des espaces\n",
    "    tweet = re.sub(r'\\r', ' ', tweet)\n",
    "    # Enlever les URLs\n",
    "    tweet = re.sub(r'http\\S+|www\\S+', '', tweet)\n",
    "    # Enlever les mentions\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    # Enlever les hashtags\n",
    "    tweet = re.sub(r'#\\w+', '', tweet)\n",
    "    # Enlever les caract√®res sp√©ciaux en gardant seulement les alphanum√©riques et les espaces\n",
    "    tweet = ''.join([char for char in tweet if char.isalnum() or char.isspace()])\n",
    "    # Convertir en minuscules\n",
    "    tweet = tweet.lower()\n",
    "    # Enlever les stopwords\n",
    "    tweet = ' '.join([word for word in tweet.split() if word not in stop_words])\n",
    "    return tweet\n",
    "\n",
    "# Appliquer le pr√©traitement √† chaque tweet\n",
    "tweets_df = dataset['Tweet'].apply(preprocess_tweet)\n",
    "\n",
    "# Afficher les tweets pr√©trait√©s\n",
    "print(tweets_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation et Pr√©paration des S√©quences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nous utilisons Tokenizer de Keras pour transformer les tweets en s√©quences de tokens. La taille du vocabulaire est augment√©e √† 10000 mots.\n",
    "* Les s√©quences sont ensuite remplies (padding) √† une longueur fixe de 120 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation : Transformer les tweets en s√©quences de tokens\n",
    "tokenizer = Tokenizer(num_words=10000)  \n",
    "tokenizer.fit_on_texts(tweets_df)\n",
    "X = tokenizer.texts_to_sequences(tweets_df)\n",
    "\n",
    "# Padding des s√©quences : Normaliser les s√©quences √† une longueur fixe\n",
    "X = pad_sequences(X, maxlen=60)  # Augmentation de la longueur maximale des s√©quences\n",
    "\n",
    "# Convertir les labels de sentiment en valeurs num√©riques\n",
    "dataset['Sentiment'] = dataset['Sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
    "y = dataset['Sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(dataset['Sentiment'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D√©finition et Entra√Ænement du Mod√®le LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nous d√©finissons un mod√®le LSTM bidirectionnel avec des couches de dropout pour r√©gulariser les donn√©es et √©viter le surapprentissage.\n",
    "* Le mod√®le est compil√© avec la perte sparse_categorical_crossentropy et l'optimiseur Adam.\n",
    "* Nous entra√Ænons le mod√®le sur les donn√©es d'entra√Ænement et validons sur les donn√©es de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# D√©finition de la classe pour le mod√®le LSTM personnalis√©\n",
    "class CustomLSTMModel:\n",
    "    def __init__(self, vocab_size, embedding_dim, input_length, hidden_dim, output_size):\n",
    "        # Initialiser le mod√®le s√©quentiel\n",
    "        self.model = Sequential()\n",
    "        # Ajouter une couche d'embedding\n",
    "        self.model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length))\n",
    "        # Ajouter un SpatialDropout pour r√©gulariser les embeddings\n",
    "        self.model.add(SpatialDropout1D(0.3)) \n",
    "        # Ajouter une couche LSTM bidirectionnelle avec dropout\n",
    "        self.model.add(Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)))\n",
    "        # Ajouter une couche LSTM avec dropout\n",
    "        self.model.add(LSTM(hidden_dim, dropout=0.3, recurrent_dropout=0.3))\n",
    "        # Ajouter une couche dense avec activation softmax pour la classification\n",
    "        self.model.add(Dense(output_size, activation='softmax'))\n",
    "        \n",
    "        # Compiler le mod√®le avec la perte 'sparse_categorical_crossentropy' et l'optimiseur Adam\n",
    "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "    # Fonction pour entra√Æner le mod√®le\n",
    "    def fit(self, X_train, y_train, validation_data, epochs=10, batch_size=64):  # Augmentation des √©poques\n",
    "        history = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data, verbose=1)\n",
    "        return history\n",
    "\n",
    "    # Fonction pour √©valuer le mod√®le\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        loss, accuracy = self.model.evaluate(X_test, y_test, verbose=1)\n",
    "        return loss, accuracy\n",
    "\n",
    "    # Fonction pour pr√©dire les classes des nouvelles donn√©es\n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test)\n",
    "\n",
    "# Assurer que les donn√©es sont sous forme de tableau numpy\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Diviser les donn√©es en ensembles d'entra√Ænement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialiser et entra√Æner le mod√®le LSTM personnalis√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1127/1127 [==============================] - 78s 62ms/step - loss: 0.8102 - accuracy: 0.6170 - val_loss: 0.6774 - val_accuracy: 0.7070\n",
      "Epoch 2/4\n",
      "1127/1127 [==============================] - 74s 66ms/step - loss: 0.5661 - accuracy: 0.7646 - val_loss: 0.6556 - val_accuracy: 0.7154\n",
      "Epoch 3/4\n",
      "1127/1127 [==============================] - 73s 65ms/step - loss: 0.4729 - accuracy: 0.8099 - val_loss: 0.7039 - val_accuracy: 0.7103\n",
      "Epoch 4/4\n",
      "1127/1127 [==============================] - 71s 63ms/step - loss: 0.4178 - accuracy: 0.8315 - val_loss: 0.7323 - val_accuracy: 0.7072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24db6d28990>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialiser et entra√Æner le mod√®le LSTM personnalis√©\n",
    "\n",
    "# Taille du vocabulaire ajust√©e\n",
    "vocab_size = 10000 \n",
    "\n",
    "# Dimension des embeddings \n",
    "embedding_dim = 50 #128 #50 \n",
    "\n",
    "# Longueur maximale des s√©quences\n",
    "input_length = 60 #120 #60\n",
    "\n",
    "# Nombre de neurones dans la couche cach√©e LSTM\n",
    "hidden_dim = 32 #128 #32\n",
    "\n",
    "# Pour trois classes : n√©gatif, neutre, positif\n",
    "output_size = 3  \n",
    "\n",
    "# Nombre d'√©poques d'entra√Ænement\n",
    "epochs= 4 #10 #4\n",
    "\n",
    "# Taille des lots\n",
    "batch_size = 16 #64 #16\n",
    "\n",
    "\n",
    "custom_lstm_model = CustomLSTMModel(vocab_size, embedding_dim, input_length, hidden_dim, output_size)\n",
    "custom_lstm_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpr√©tation :\n",
    "\n",
    "* La perte d'entra√Ænement diminue bien, atteignant 0.4187, et l'exactitude d'entra√Ænement augmente √† 83.17%.\n",
    "* La perte de validation continue d'augmenter l√©g√®rement, atteignant 0.7109, ce qui renforce l'indication de surapprentissage.\n",
    "* L'exactitude de validation reste stable √† 70.74%, indiquant que malgr√© l'augmentation de la performance sur les donn√©es d'entra√Ænement, le mod√®le ne s'am√©liore pas sur les donn√©es de validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âvaluation et Pr√©diction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nous √©valuons le mod√®le sur les donn√©es de test pour obtenir la perte et la pr√©cision.\n",
    "* Nous pr√©disons les sentiments des tweets de test et affichons un rapport de classification pour √©valuer les performances du mod√®le.\n",
    "* Pour comparaison, nous entra√Ænons √©galement un mod√®le de r√©gression logistique et comparons ses performances avec le mod√®le LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 2s 11ms/step - loss: 0.7323 - accuracy: 0.7072\n",
      "Test Loss: 0.7323295474052429 | Test Accuracy: 0.7072141766548157\n",
      "141/141 [==============================] - 2s 11ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.72      0.72      1149\n",
      "     neutral       0.67      0.62      0.65      1648\n",
      "    positive       0.72      0.78      0.75      1708\n",
      "\n",
      "    accuracy                           0.71      4505\n",
      "   macro avg       0.71      0.71      0.71      4505\n",
      "weighted avg       0.71      0.71      0.71      4505\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "Logistic Regression Accuracy: 0.3908990011098779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.27      0.03      0.05      1149\n",
      "     neutral       0.37      0.36      0.37      1648\n",
      "    positive       0.41      0.67      0.50      1708\n",
      "\n",
      "    accuracy                           0.39      4505\n",
      "   macro avg       0.35      0.35      0.31      4505\n",
      "weighted avg       0.36      0.39      0.34      4505\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cypri\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# √âvaluer le mod√®le\n",
    "loss, accuracy = custom_lstm_model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss} | Test Accuracy: {accuracy}')\n",
    "\n",
    "# Pr√©dire et imprimer le rapport de classification\n",
    "y_pred = np.argmax(custom_lstm_model.predict(X_test), axis=-1)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['negative', 'neutral', 'positive']))\n",
    "\n",
    "# Mod√®le de r√©gression logistique pour comparaison\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Entra√Æner le mod√®le de r√©gression logistique\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Pr√©dire et √©valuer\n",
    "y_pred_logistic = logistic_model.predict(X_test)\n",
    "print(\"----------------------------------------------------------------------------\")\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_logistic))\n",
    "print(classification_report(y_test, y_pred_logistic, target_names=['negative', 'neutral', 'positive']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpr√©tation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Mod√®le LSTM</u>:\n",
    "\n",
    "* L'exactitude de test de 70.74% est coh√©rente avec l'exactitude de validation observ√©e pendant l'entra√Ænement, sugg√©rant que le mod√®le g√©n√©ralise bien aux donn√©es de test.\n",
    "  * Test Loss (Perte de Test) : 71.09%\n",
    "  * Test Accuracy (Exactitude de Test) : 70.74%\n",
    "\n",
    "* La classe \"positive\" a la meilleure pr√©cision, tandis que la classe \"neutral\" a le meilleur rappel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>R√©gression logistique</u>:\n",
    "\n",
    "* L'exactitude du mod√®le de r√©gression logistique est bien inf√©rieure √† celle du mod√®le LSTM, indiquant que la r√©gression logistique a du mal √† capturer les relations complexes dans les donn√©es de tweets.\n",
    "\n",
    "* La performance de la r√©gression logistique est particuli√®rement faible pour la classe \"negative\", avec un rappel de seulement 3%. Cela sugg√®re que le mod√®le de r√©gression logistique ne parvient pas √† capturer les caract√©ristiques des tweets n√©gatifs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "L'exactitude du mod√®le de r√©gression logistique est bien inf√©rieure √† celle du mod√®le LSTM, indiquant que la r√©gression logistique a du mal √† capturer les relations complexes dans les donn√©es de tweets.\n",
    "* Test Accuracy R√©gression Logistique : 39.09%\n",
    "* Test Accuracy Mod√®le LSTM : 70.74%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>ConvergenceWarning </u>:\n",
    "\n",
    "L'avertissement indique que l'optimiseur L-BFGS a atteint le nombre maximum d'it√©rations avant de converger. Pour r√©soudre ce probl√®me, il est possible :\n",
    "\n",
    "* D'augmenter le nombre d'it√©rations (max_iter).\n",
    "* De pr√©traiter les donn√©es en les normalisant ou en les standardisant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
